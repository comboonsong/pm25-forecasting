{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 23:45:53.583132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738341953.598439   30777 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738341953.603023   30777 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-31 23:45:53.623259: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "df = con.read_parquet(\"data/pm25*.parquet\").df()\n",
    "\n",
    "# Add 'day' and 'hour' columns from the timestamp\n",
    "df.insert(loc=3, column=\"day\", value=df[\"timestamp\"].dt.day)\n",
    "df.insert(loc=4, column=\"hour\", value=df[\"timestamp\"].dt.hour)\n",
    "\n",
    "# Remove timezone info from the timestamp\n",
    "df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(None)\n",
    "\n",
    "# Preprocessing\n",
    "# Assuming we have features like 'temperature', 'humidity', 'wind_speed', etc.\n",
    "features = [\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"temperature\",\n",
    "    \"humidity\",\n",
    "    \"pressure\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\",\n",
    "    \"pm2_5\",\n",
    "]\n",
    "df = df[features + [\"pollution_level\"]].copy()\n",
    "\n",
    "# Mapping pollution levels to numerical values\n",
    "pollution_level_mapping = {\n",
    "    \"คุณภาพอากาศดีมาก\": 0,  # Very Good\n",
    "    \"คุณภาพอากาศดี\": 1,     # Good\n",
    "    \"ปานกลาง\": 2,            # Moderate\n",
    "    \"เริ่มมีผลกระทบต่อสุขภาพ\": 3,  # Unhealthy for Sensitive Groups\n",
    "    \"มีผลกระทบต่อสุขภาพ\": 4    # Unhealthy\n",
    "}\n",
    "\n",
    "# Apply mapping to pollution_level\n",
    "df['pollution_level_class'] = df['pollution_level'].map(pollution_level_mapping)\n",
    "\n",
    "# Sort the DataFrame\n",
    "df = df.sort_values(by=['year', 'month', 'day', 'hour'], ascending=True)\n",
    "# prompt: keep only one record for each row\n",
    "\n",
    "# Check for duplicates based on all columns except 'pollution_level'\n",
    "duplicates = df[df.duplicated(subset=features, keep=False)]\n",
    "\n",
    "# If duplicates exist, keep only the first occurrence\n",
    "if not duplicates.empty:\n",
    "    df = df.drop_duplicates(subset=features, keep='first')\n",
    "\n",
    "# Filter the data into train and test based on year\n",
    "train_data = df[df['year'].isin([2021, 2022, 2023])]\n",
    "test_data = df[df['year'] == 2024]\n",
    "#test_data = df[df['year'].isin([2023, 2024])]\n",
    "\n",
    "# Scaling features except for the target ('pollution_level_class')\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train_features = scaler.fit_transform(train_data[features])\n",
    "scaled_test_features = scaler.transform(test_data[features])\n",
    "\n",
    "# Encode target labels to categorical\n",
    "y_train_encoded = to_categorical(train_data['pollution_level_class'])\n",
    "y_test_encoded = to_categorical(test_data['pollution_level_class'])\n",
    "\n",
    "# Adjust the sequence creation function for 12-hour prediction\n",
    "def create_sequences(data, target, look_back, predict_ahead):\n",
    "    X, y = [], []\n",
    "    for i in range(48,len(data) - look_back - predict_ahead + 1):\n",
    "        X.append(data[i:i + look_back, :])  # Use all features for past `look_back`\n",
    "        y.append(target[i + look_back:i + look_back + predict_ahead])  # Predict `predict_ahead` steps\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 48  # Past 20 time steps\n",
    "predict_ahead = 12  # Predict the next 12 hours\n",
    "\n",
    "# Create sequences\n",
    "X_train, y_train = create_sequences(scaled_train_features, y_train_encoded, look_back, predict_ahead)\n",
    "X_test, y_test = create_sequences(scaled_test_features, y_test_encoded, look_back, predict_ahead)\n",
    "# X_train, y_train = create_sequences(train_data[features].values, y_train_encoded, look_back, predict_ahead)\n",
    "# X_test, y_test = create_sequences(train_data[features].values, y_test_encoded, look_back, predict_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the focal loss function\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # Clip y_pred to avoid log(0) error\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        # Calculate focal loss for each time step using one-hot encoded labels\n",
    "        # tf.where requires y_true to have the same shape as y_pred\n",
    "\n",
    "        # Calculate cross-entropy loss\n",
    "        ce_loss = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Apply focal weights\n",
    "        loss = alpha * K.pow(1. - y_pred, gamma) * ce_loss\n",
    "\n",
    "        # Modification: Sum over the last axis to get a single loss value\n",
    "        loss = K.sum(loss, axis=-1)\n",
    "\n",
    "        # Average loss over the time dimension\n",
    "        return K.mean(loss)  # Calculate the mean over the remaining dimensions\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: add focal loss to lstm model and add precision and recall matrix\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=y_train.shape[1] * y_train.shape[2]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Reshape((y_train.shape[1], y_train.shape[2])))\n",
    "model.add(Dense(units=y_train.shape[2], activation='softmax'))  # Output layer with softmax\n",
    "\n",
    "# Compile the model with focal loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss=focal_loss(alpha=0.25, gamma=2),\n",
    "              #loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "# Add a model checkpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Assuming 'history' is the output of model.fit()\n",
    "# Plotting training and validation loss\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (rest of your model training code) ...\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved best model\n",
    "model = load_model(\n",
    "    './best_model.keras',\n",
    "    custom_objects={'focal_loss_fixed': focal_loss(alpha=0.25, gamma=2)}  # Add custom objects if required\n",
    ")\n",
    "\n",
    "y_test_classes = np.argmax(y_test, axis=2) + 1\n",
    "# Assuming y_test and y_pred are of shape (samples, timesteps, classes)\n",
    "y_pred = model.predict(X_test)\n",
    "# y_pred_classes = (y_pred > 0.4).astype(int)  # Apply threshold (adjust if needed)\n",
    "y_pred_classes = np.argmax(y_pred, axis=2) + 1\n",
    "\n",
    "# Reshape to (samples * timesteps, classes) for sklearn metrics\n",
    "# y_test_flat = y_test.reshape(-1, y_test.shape[-1])\n",
    "# y_pred_flat = y_pred_classes.reshape(-1, y_pred_classes.shape[-1])\n",
    "\n",
    "# # Calculate and print the classification report\n",
    "# print(classification_report(y_test_flat, y_pred_flat))\n",
    "\n",
    "# # Calculate the multilabel confusion matrix\n",
    "# cm = multilabel_confusion_matrix(y_test_flat, y_pred_flat)\n",
    "\n",
    "# # Plot confusion matrices for each label\n",
    "# num_labels = y_test.shape[-1]  # Get the number of labels\n",
    "# fig, axes = plt.subplots(num_labels, 1, figsize=(8, 6 * num_labels))\n",
    "\n",
    "# for i in range(num_labels):\n",
    "#     sns.heatmap(cm[i], annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i])\n",
    "#     axes[i].set_xlabel('Predicted')\n",
    "#     axes[i].set_ylabel('True')\n",
    "#     axes[i].set_title(f'Confusion Matrix for Label {i}')\n",
    "\n",
    "# plt.tight_layout()  # Adjust spacing between subplots\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Calculate MAPE using y_test_classes and y_pred_classes\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test_classes, y_pred_classes)\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Calculate Recall, Precision, F1, accuracy using y_test_classes and y_pred_classes\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_test_classes and y_pred_classes are defined as in your provided code\n",
    "\n",
    "# Flatten the arrays for sklearn metrics if they are not already flattened\n",
    "y_test_classes_flat = y_test_classes.flatten()\n",
    "y_pred_classes_flat = y_pred_classes.flatten()\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test_classes_flat, y_pred_classes_flat)\n",
    "precision = precision_score(y_test_classes_flat, y_pred_classes_flat, average='macro') # Use macro averaging for multi-class\n",
    "recall = recall_score(y_test_classes_flat, y_pred_classes_flat, average='macro') # Use macro averaging for multi-class\n",
    "f1 = f1_score(y_test_classes_flat, y_pred_classes_flat, average='macro') # Use macro averaging for multi-class\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pm25-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
